{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM 336546 - HW1: Fetal Cardiotocograms \n",
    "# Part I: Data Exploration\n",
    "\n",
    "In this homework you will be working on predicting fetal outcomes from continuous labor monitoring using Cardiotography (CTG). In particular you will use measures of the fetal heart rate (FHR) and use these as input features to your linear classifier. Before we dive into the assignment itself, let's make a quick introduction to CTG and get familiar with this type of examination and its underlying physiological basis as a good biomedical engineer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrapartum CTG is used routinely to measure maternal uterine pressure and fetal heart rate (FHR). Antepartum CTG monitoring is used to identify fetuses at risk of intrauterine hypoxia and academia. As early as 28 weeks of gestation, analysis of the FHR trace is used as a non-stress test to assess the fetal well-being. In the perinatal period, timely, appropriate intervention can avoid fetal neurological damage or death. The CTG is visually assessed by a clinician or interpreted by computer analysis. In the context of labor monitoring, the CTG is used for continuous fetal monitoring. An abnormal heart rate will lead the clinician to perform a cesarean. We will focus on CTG monitoring during labor in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CTG has two different transducers: One of them is a transducer placed on the mother’s abdomen, above the fetal heart, to monitor heart rate using Doppler probe (cardiogram). The other is located at the fundus of the uterus to measure frequency of contractions (tocogram). Tocodynamometry is a strain gauge technology provides contraction frequency and approximate duration of labor contractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a lot of features from a CTG. Here are some of them:\n",
    "* **Uterine activity**: Duration, frequency and intensity of contractions.\n",
    "* **Baseline FHR**: Mean FHR rounded to increments of 5 beats per minute (bpm) during a 10-minute window.\n",
    "* **Baseline FHR variability**: Fluctuations in the baseline FHR that are irregular in amplitude and frequency.\n",
    "* **Presence of accelerations**: A visually apparent abrupt increase in fetal heart rate.\n",
    "\n",
    "Here is an example of a typical CTG with some of its features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/6ed5ef1da100ebc2241c3a3945e1da9ce79f73ac/1-Figure1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some important notes before we begin* - \n",
    "> - *Freeze  your randomness using random.seed().*\n",
    "\n",
    "> - *Check for each figure $-$ labels, title and units. If something is missing, add it.*\n",
    "\n",
    "> - *In this assignment you are not allowed to use packages that are not in the given environment. Don't add \"import...\".*\n",
    "\n",
    "> - *Keep your code commented and tidy. Google is by your side, use it and explore existing functions.*\n",
    "\n",
    "> - ***Before submission, check that your code is fully running on this Jupyter notebook.***\n",
    "\n",
    "The CTG dataset is an Excel file which was sent to you. For more information, please look at the Excel sheet called Description or take a look at this [link](http://archive.ics.uci.edu/ml/datasets/Cardiotocography). Our main goal in this assignment is to train an algorithm to decide what is the fetal state according to the extracted features. Before we even start dealing with the data itself, we should apply the first and most important rule of data/signal processing: **ALWAYS LOOK AND UNDERSTAND THE DATA FIRST!**\n",
    "In order to do that, we will load the file into a variable called `CTG_features` and use descriptive statistics and visualization tools you have seen in the lectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "%load_ext autoreload\n",
    "\n",
    "file = Path.cwd().joinpath('messed_CTG.xls') # concatenates messed_CTG.xls to the current folder that should be the extracted zip folder \n",
    "CTG_dataset = pd.read_excel(file, sheet_name='Raw Data').iloc[1:, :]  # load the data and drop the first row that was filled with nans\n",
    "CTG_features = CTG_dataset[['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DR', 'DP', 'ASTV', 'MSTV', 'ALTV', 'MLTV',\n",
    " 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean', 'Median', 'Variance', 'Tendency']]\n",
    "CTG_morph = CTG_dataset[['CLASS']]\n",
    "fetal_state = CTG_dataset[['NSP']]\n",
    "\n",
    "random.seed(10)  # fill your seed number here\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First  look at the data in your Excel file. You can see that in some of the cells we have '--' or NaN etc. Furthermore, the description tells us that the feature `DR` was removed although we did load it into our dataset.\\\n",
    "Your first task is: Implement the function `rm_ext_and_nan` in the module  `clean_data` so it will remove the extra feature `DR` (ignore the feature), and all non-numeric values (ignore the samples). Notice that removing nan should be performed for every feature. Do not remove an entire row. This function should return a dictionary of features where the values of each feature are the clean excel columns without the `DR` feature. **Hint**: In order to eliminate every cell that is non-numeric, you will have to transform it first to NaN and only then eliminate them. **Note**: `CTG_dataset` is a `pandas DataFrame` and every element within it is called `pandas series` (a table column). For our use, you can treat a dataframe as if it was a dictionary so that every key has a pandas series as value.\\\n",
    "**Bonus**:  Implement the function in a single line of code using dictionary comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import rm_ext_and_nan as rm\n",
    "\n",
    "extra_feature = 'DR' \n",
    "c_ctg = rm(CTG_features, extra_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and make sure that your function works well by comparing the histograms' width feature. First, we will plot the original distribution of this feature where every NaN element was replaced by a value that is not reasonable as 1000 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feat = 'Width'\n",
    "Q = pd.DataFrame(CTG_features[feat])\n",
    "idx_na = Q.index[Q[feat].isna()].tolist()\n",
    "for i in idx_na:\n",
    "    Q.loc[i] = 1000\n",
    "Q.hist(bins = 100)\n",
    "plt.xlabel('Histogram Width')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following lines of code to check how you performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat = 'Width'\n",
    "Q_clean = pd.DataFrame(c_ctg[feat])\n",
    "Q_clean.hist(bins=100)\n",
    "plt.xlabel('Histogram Width')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are warmed up, let's do something a bit different. Instead of removing the NaN values, handle those missing values using random sampling of each series values. Use np.random.choice to specify each replacing-values' probability, so that the random sampling will be derived from the same distribution as the original valid series data. Again, first convert all non-numeric values to NaN and only then apply the sampling method. Implement the function `nan2num_samp`. ***Don't forget to remove `DR` again.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ofeka\\anaconda3\\envs\\bm-336546\\lib\\site-packages\\pandas\\core\\frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'set_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-049d042705f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mextra_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DR'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mc_samp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnan2num_samp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCTG_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Projects\\Machine Learning\\Homework\\HW1\\clean_data.py\u001b[0m in \u001b[0;36mnan2num_samp\u001b[1;34m(CTG_features, extra_feature)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# random sampling of len(idx) values from col\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bm-336546\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5138\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'set_value'"
     ]
    }
   ],
   "source": [
    "from clean_data import nan2num_samp\n",
    "\n",
    "extra_feature = 'DR' \n",
    "c_samp = nan2num_samp(CTG_features, extra_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following lines of code to check how you performed for example with the feature `MSTV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'MSTV'\n",
    "print(CTG_features[feat].iloc[0:5]) # print first 5 values\n",
    "print(c_samp[feat].iloc[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our \"clean\" data using histograms, barplots and boxplots and then refer to the following questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "c_samp.boxplot(column=['Median','Mean','Mode'])\n",
    "plt.ylabel('Fetal Heart Rate [bpm]')\n",
    "\n",
    "# Histograms\n",
    "xlbl = ['beats/min','1/sec','1/sec','%']\n",
    "axarr = c_samp.hist(column=['LB', 'AC', 'UC','ASTV'], bins=100,layout = (2, 2),figsize=(20, 10))\n",
    "for i,ax in enumerate(axarr.flatten()):\n",
    "    ax.set_xlabel(xlbl[i])\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    \n",
    "# Barplots (error bars)\n",
    "df = pd.DataFrame.from_dict({'lab':['Min','Max'], 'val':[np.mean(c_samp['Min']), np.mean(c_samp['Max'])]})\n",
    "errors = [np.std(c_samp['Min']), np.std(c_samp['Max'])] \n",
    "ax = df.plot.bar(x='lab', y='val', yerr=errors, rot=0)\n",
    "ax.set_ylabel('Average value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**: \n",
    "> * Please answer all of the following questions within the notebook itself. Remember that the only files you will submit are the notebook and the fully-implemented `.py` files.\n",
    "\n",
    "> * Do not change the notebook's cells unless you were specifically told to (such as the \"Answers\" cells etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q1:** What information can you get from histograms and what information can you get from boxplots?\n",
    "\n",
    "**Q2:** Error bars can be misleading. In what sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q1: \n",
    "Histograms can give information about the distribution of the data- If the data is from a known distribution, than the histogram is expected to have a shape (or envelope) that resambles the original known distribution. Histograms also show which values in the data are more common and which are not, and if there are abnormal values (but boxplot may be better for the second aim).\n",
    "Box plots provide basic statistic information about the data- median, first and third quartiles, and outliers. A lot of information about the data can be deduced from it. for example, if the interquartile range in large, one can know that the data is very scattered. The quality of the data can also be estimated according to the amount of outliers.**\n",
    "\n",
    "\n",
    "**Q2: First, sometimes it is unclear what the error bars represent- if it is STD the conclusion will be different than if it is MSE. Second, they are influenced by the data- if we have 2 datasets, one with outliers and one without outliers, the barplot may look similar when the datasets are actually very different.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have visualized  the data, cleaned it and obtained some insights from it, we would like to compute the summary statistics for each feature. Implement the `sum_stat` function which returns a dictionary of dictionaries, meaning that a key value of a feature will return a dictionary with keys of min, Q1, median, Q3, max.\n",
    "It should look something like this:\n",
    "\n",
    "d_summary = {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"MSTV\": {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"min\": 2.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q1\": 3.0,<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"median\": 4.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q3\": 5.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"max\": 6.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;},<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"LB\": {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"min\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q1\": ...,<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"median\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q3\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"max\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;},<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;}<br>\n",
    "<br> \n",
    "E.g. to access 2.0 above write : d_summary[\"MSTV\"][\"min\"] = 2.0\n",
    "\n",
    "You can use that output in order to have another cleanup and this time, it will be a cleanup of outliers. We will stick to the definition of an outlier according to the 'five number summary' that are actually represented by boxplots. Just as a reminder and comparison to a normal distribution, have a look at the next figure:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*NRlqiZGQdsIyAu0KzP7LaQ.png\" width=\"400\"><\\center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import sum_stat as sst\n",
    "\n",
    "d_summary = sst(c_samp)\n",
    "print(d_summary['MSTV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `rm_outlier` that will have the output of `sum_stat` as an input and will return a dictionary (similar to `c_ctg`) that will have outliers removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import rm_outlier\n",
    "\n",
    "c_no_outlier = rm_outlier(c_samp, d_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the features `Median`, `Mean` and `Mode` for comparison previous to outliers removal and after it using boxplots. First we plot the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_samp.boxplot(column=['Median', 'Mean', 'Mode'])\n",
    "plt.ylabel('Fetal Heart Rate [bpm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the \"clean data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_no_outlier.boxplot(column=['Median', 'Mean', 'Mode'])\n",
    "plt.ylabel('Fetal Heart Rate [bpm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q3:** Boxplotting the data after the outliers were removed shows us that there are no outliers anymore. Is it necessarily always the case, meaning if you take the \"clean\" data and boxplot it again will it have no outliers for sure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q3: It is not necessarily always the case, because the calculation of outliers is relative to the complete dataset. Values that were not considered outliers before the removal may be considered outliers after it, because the distribution has changed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is one more thing that you should be reminded of in respect to data exploration and it is the second rule of this field that states the following: **USE COMMON SENSE!**\n",
    "\n",
    "What it really means is that if you have some physiological prior (e.g. you know the range of values of your features), so you should have some sanity checks. For example, the feature `LB` tells us about the FHR baseline. It won't make any sense if we found values that are higher than 500 bpm, not even mentioning non-positive values. Your next mission is to implement the function `phys_prior` where you choose one feature (which is not `LB`), explain what you think it's normal range is and clean it accordingly. The function will have `c_samp`, the feature that you chose and a threshold as inputs. The explanation should be written shortly as a comment next to the input as you can see at the following cell. The only lines you can change here is the `feature` value, the `thresh` value and its comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import phys_prior as phpr\n",
    "\n",
    "feature = 'Max'  # change this feature\n",
    "thresh = 250  # maximum of FHR histogram cannot physiologically be above 250 bpm\n",
    "filt_feature = phpr(c_samp, feature, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling: Standardization and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point you have successfully cleaned your dataset, well done! The clean dataset was saved in pickle format called `objs.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('objs.pkl', 'rb') as f:\n",
    "    CTG_features, CTG_morph, fetal_state = pickle.load(f)\n",
    "orig_feat = CTG_features.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will address an important step in data science which is called feature scaling. Here we will discuss about standardization and normalization. As you saw in the lectures, scaling enables us to prepare features that take their value in different ranges and map them to a “normalized” features that take their values in similar ranges.\n",
    "\n",
    "Implement the function `norm_standard` that will have four inputs: `data`, `selected_feat`, `mode` and `flag`. The function will return the **whole data** normalized/standardized by series according to *mode*, but you should also choose two features for visualized comparison (using histograms) between the original data and the different modes. Use `matplotlib` as you saw in your tutorials. The argument `flag` is used for visibility of histograms.  There are three types of `mode`: `'standard','MinMax' and 'mean'`. Look for their meanings in the second lecture, slides 46-47. The first call uses `mode=none` and `flag=False` (as defaults). Don't change this default. The only variable you are allowed to change in the next call is `selected_feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import norm_standard as nsd\n",
    "\n",
    "selected_feat = ('LB','ASTV')\n",
    "orig = nsd(CTG_features, selected_feat, flag=True)\n",
    "nsd_std = nsd(CTG_features, selected_feat, mode='standard', flag=True)\n",
    "nsd_norm = nsd(CTG_features, selected_feat, mode='MinMax', flag=True)\n",
    "nsd_norm_mean = nsd(CTG_features, selected_feat, mode='mean', flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q4:** Explain why normalization is not useful when there are outliers with extremely large or small values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q4: Normalization is based on values that are very influenced by extremely large or small values. Min-Max scaling and mean sacling are based on the min and max values (mean sacling also uses mean which is sensitive to outliers too). Therefore, using normalization in such case will distort the data (compress it), and will lead to misleading results.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after all of the hard work we can now harvest the fruits (your functions from Part I) in order to do some proper machine learning!  \n",
    "\n",
    "Note: It is recommended that you attend the second workshop for this part and use the notes in your homework folder.\n",
    "\n",
    "In this assignment we will assume that our data is linearly separable and we will use logistic regression as our classification method. In other words, we choose a linear hypothesis class function . We would try to make the separation in the feature domain (i.e. our graph axes are the features) and we will have a multiclass problem.\n",
    "\n",
    "For every sample (example as called in the lecture) we have two types of labels and we will deal only with one of them. Our goal is to learn the function that gets a sample as an input and returns a predicted value which is supposed to be the class (label) that it belongs to. Our type of label will be `fetal_state`. Before we continue towards the \"learning\" part we will have another look at our data. Starting with our labels distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "g = sns.countplot(x = 'NSP', data = fetal_state)\n",
    "g.set(xticklabels=['Normal','Suspect','Pathology'])\n",
    "plt.show()\n",
    "idx_1 = (fetal_state == 1).index[(fetal_state == 1)['NSP'] == True].tolist()\n",
    "idx_2 = (fetal_state == 2).index[(fetal_state == 2)['NSP'] == True].tolist()\n",
    "idx_3 = (fetal_state == 3).index[(fetal_state == 3)['NSP'] == True].tolist()\n",
    "print(\"Normal samples account for \" + str(\"{0:.2f}\".format(100 * len(idx_1) / len(fetal_state))) + \"% of the data.\")\n",
    "print(\"Suspect samples account for \" + str(\"{0:.2f}\".format(100 * len(idx_2) / len(fetal_state))) + \"% of the data.\")\n",
    "print(\"Pathology samples account for \" + str(\"{0:.2f}\".format(100 * len(idx_3) / len(fetal_state))) + \"% of the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the CTG's were labeled as normal. Mostly, labels are made by professionals (in our case, doctors) based on the interpretation of the FHR and our job is to make the computer make the same decisions as a doctor would do but automatically. Now let's get the feeling of how well the features correlate with the labels and with one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 100\n",
    "feat = 'Width'\n",
    "plt.hist(CTG_features[feat].loc[idx_1], bins, density=True, alpha=0.5, label='Normal')\n",
    "plt.hist(CTG_features[feat].loc[idx_2], bins, density=True, alpha=0.5, label='Suspect')\n",
    "plt.hist(CTG_features[feat].loc[idx_3], bins, density=True, alpha=0.5, label='Pathology')\n",
    "plt.xlabel('Histigram Width')\n",
    "plt.ylabel('Probabilty')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(CTG_features[['LB','AC','FM','UC']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q5:** What information does feature-feature correlation provide and what feature-target (mRMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q5: In the feature-feature correlation, the diagonal displays the histograms of the features and the rest of the graphs show if there is a linear correlation between each pair of features, and wether it is a negative or a positive correlation. Sometimes when there is a strong correlation between features, some of them can be neglected as they do not contribute more information and can lead to misleading results** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we are pretty much done with data exploration. Now the learning part begins.\\\n",
    "As you saw in the tutorials, one of the most common and useful packages that Python has to offer in the learning field is `sklearn` package. The first thing you need to do after exploring your data is to divide it into 2 sets: `training set` and `testing set`. As a rule of thumb, a typical split of your dataset is 80%-20%,  respectively. Later on we will also use validation set.\\\n",
    "One of the most common linear classification models is *logistic regression* – abbreviated ‘LR’. We will use this model through our assignment from now on.\\\n",
    "Implement the function `pred_log` which is in the module `lin_classifier`. It should return a tuple of two elements. The first one is a vector of `predicted classes` and the other one is the weighting matrix (`w`) that should have the shape of (# of classes, # of features). We will use the one-vs-rest (ovr) form of multinomial LR in order to interpret the results using odds ratio later on.\n",
    "\n",
    "As you noticed, most of our data is labeled as \"Normal\" which means that our data is *imbalanced*. This can cause **biased learning**. This is the reason why we *stratification* when we split out data to train and test sets. Stratification means that the split preserves the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lin_classifier import *\n",
    "\n",
    "orig_feat = CTG_features.columns.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(CTG_features, np.ravel(fetal_state), test_size=0.2, random_state=0, stratify=np.ravel(fetal_state))\n",
    "logreg = LogisticRegression(solver='saga', multi_class='ovr', penalty='none', max_iter=10000)\n",
    "y_pred, w = pred_log(logreg, X_train, y_train, X_test)\n",
    "\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test, y_pred, average='macro'))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you got about 88% accuracy. Not bad! This is a good result compared to the naive classifier (predicting a \"Normal\" label for every sample) that should have given us around 77% accuracy (Why?). \n",
    "\n",
    "Now we'll practice an interpretation of the results. As you saw in lecture 4, odds ratio can be applied on binary LR, and here we'll expand it to the multinomial (ovr) case. We want to explore how addition of +1 to a specific feature affect the 'Normal' labeling incidence. Implement the function `odds_ratio` under lin_classifier. The function inputs are weights (`w`), X and the selected feature. Regarding the second argument, think which one fits - X_train or X_test and fill this argument accordingly. Choose one of the features as you wish and return both the `odds ratio` and the median `odd`.\n",
    "\n",
    "*Hint:*\n",
    "*First try to understand the relation between binary LR and one-vs-rest LR.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat = 'LB'\n",
    "\n",
    "odds, ratio = odds_ratio(w, X_train, selected_feat=selected_feat)  # you have to fill the right X first\n",
    "\n",
    "print(f'The odds ratio of {selected_feat} for Normal is {ratio}')\n",
    "print(f\"The odds to be labeled as 'Normal' is {odds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "**Q6:** What is the meaning of your results? Explain the difference between odds_ratio and odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q6: The odds ratio represents how the odds change with one unit increase in a variable, holding all other variables constant. In our case, with one unit increase in 'LB', the odds to be labeled as 'Normal' increase by 3.78%. The odds represents the likelihood to get a certain label over the other lables. In our case, the odds of being labled as 'normal' over being labled as 'suspect' or 'pathology' are 15:1.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if normalization and standardization help us. Fill the next cell and print the three accuracies of the standardized and normalized training and testing data. *Important notes*:\n",
    "\n",
    "* Avoid information leakage! (from the test set to the train set)\n",
    "* Do not apply the `norm_standard (nsd)` function on the labels. \n",
    "* Set the `flag` argument to `False` when using `nsd` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your code here:\n",
    "mode = 'MinMax'  # choose a mode from the `nsd` \n",
    "y_pred, w_norm_std = pred_log(logreg, nsd(X_train, mode=mode), y_train, nsd(X_test, mode=mode))  # complete this function using nsd function\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test, y_pred, average='macro'))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose now one of the training-testing dataset and stick to it. Let's visualize our learned parameters. Use your chosen weight matrix as an input to the function `w_no_p_table` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mat = w_norm_std  # Fill this argument\n",
    "w_no_p_table(input_mat,orig_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q7:** Mention one advantage of using cross entropy as the cost function.\n",
    "\n",
    "\n",
    "**Q8:** By selecting one feature at a time and compare their learned weights by looking at plots we had, what can you tell about the weights relations? Why does it happen? **Hint:** notice the sign of the weights and remember that an exponent is a monotonic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q7: Cross entropy function (or log loss) penalizes the model more (than the gradient decend function for example). A bigger penalty is good for logistic regression because the classification is binary. Incorrect classification is a more serious error than a value with a certain percentage deviation, So we want an error to cost more.**\n",
    "\n",
    "**Q8: In general, there is a negative relation between the 'normal' label and the 'pathology' label, because they represent opposite cases. The 'suspect' label has an unambiguous connection to the other labels, which makes sense because this label is inherently ambiguous. Some weights have higher absolute value- that means they have a bigger influence on classification. Positive weights raise the probality towards 1, while negative weights will lower it to 0. For example, the weights for the feature 'AC' are large, so it has a high influence on the classification, and it has a positive value for 'normal' and negative for 'suspect' and 'pathology', which means it pushes the classification towards 'normal' while pushing it away from 'suspect' and 'pathology'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's recall that in the lecture you saw that accuracy is not always our best measure. Sensitivity and specificity can be much more informative and important mostly. The choice to train a model to have better results in sensitivity aspect rather than specificity or vice versa depends on our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal','Suspect','Pathology'], yticklabels=['Normal','Suspect','Pathology'])\n",
    "ax.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q9:** What do you think is more important to us with this data? What is the clinical risk/cost for False Positive (FP) and False Negative (FN)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q9: We think that sensitivity is most important Because an accurate diagnosis is crucial for saving lives if the patient is in danger. By putting sensitivity as the most important measure we are saying we want to avoid false negative (FN) diagnosis, which means to avoid missing patients who really need help. That naturally means we are willing to risk getting more false positives (FP). The biggest problem of FP is that an inappropriate treatment may be given to healthy patient, but a more common risk is probably a waste of hospital resources (performing additional unnecessary tests, etc.), which is (in our opinion) a reasonable risk**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we will try to handle one of the main issues in learning which is called **overfitting** and one way to deal with it is called **regularization**.\n",
    "\n",
    "There are several types of regularizations and in this assignment we will experience two of them:\n",
    "\n",
    "1) Loss regularization.\n",
    "\n",
    "2) Validation.\n",
    "\n",
    "The loss function is a function that takes the predicted values and the labels and *measures* how \"close\" they are to each other. Our demand is that this \"distance\" (metric) would be as low as possible. In addition to it, we can add more \"demands\" that can be represented mathematically. For example, we can demand that the number of coefficients won't get to large or we can try to restrict their values. A more physical example is a demand that our signal has to be smooth. When we try to minimize the new loss function we actually try to find the result which is the compromise of our demands. We can also \"favor\" one demand over another using **regularization parameters**.\n",
    "\n",
    "You saw in the lecture \"demands\" on the learned weights and represented those demands mathematically using $ L_1 $ and $ L_2 $ norms. The regularization parameter was denoted as $\\lambda$ (please notice that sometimes it is common to use the notation of $ c $ where $\\lambda = c^{-1}$). Now it's your turn to become artists! Change and/or add arguments to *LogisticRegression* class in the next cell and perform learning using two regularizations: $ L_1 $ and $ L_2 $. Examine your results using the confusion matrix as we did before. Tune your regularization parameter until you get a result that you think is reasonable and that brings the sensitivity/specificity (depending on what you chose before) to the maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your code here:\n",
    "mode = 'MinMax'# choose a mode from the `nsd`\n",
    "lmbda = 0.1\n",
    "\n",
    "logreg_l2 = LogisticRegression(solver='saga', C=1/lmbda, multi_class= 'multinomial',  max_iter=10000, penalty='l2')  # complete the arguments for L2\n",
    "y_pred_2, w2 = pred_log(logreg_l2, nsd(X_train, mode=mode), y_train,\n",
    "                        nsd(X_test, mode=mode))  # complete this function using nsd function\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred_2)\n",
    "ax1 = plt.subplot(211)\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal','Suspect','Pathology'], yticklabels=['Normal','Suspect','Pathology'])\n",
    "ax1.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "\n",
    "logreg_l1 = LogisticRegression(solver='saga', C=1/lmbda, multi_class='multinomial', max_iter=10000, penalty='l1') # complete the arguments for L1\n",
    "y_pred_1, w1 = pred_log(logreg_l1, nsd(X_train, mode=mode), y_train,\n",
    "                        nsd(X_test, mode=mode))  # complete this function using nsd function\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred_1)\n",
    "ax2 = plt.subplot(212)\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal','Suspect','Pathology'], yticklabels=['Normal','Suspect','Pathology'])\n",
    "ax2.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are happy with your results, let's compare the coefficients of the two norms.\\\n",
    "Choose two weighting matrices (one calculated using $ L_2 $ and the other calculated using $ L_1 $) and use them as inputs in `w_all_tbl` function. This function sorts the weights according to their $ L_2 $ norm (so the first argument has to be the matrix of $ L_2 $) and compares them to $L_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_all_tbl(w2, w1, orig_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the features are ordered differently because they are sorted according to $ L_2 $ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q10:** What is the difference that you can see when plotting $ L_1 $ vs. $ L_2 $? Could you expect it ahead?\n",
    "\n",
    "**Q11:** From the feature analysis, which of the features are most suggestive of fetuses at risk (pathology) versus normal? Elaborate on the meaning of these features in relation to the underlying physiology. You might want to have a look at the following [link](http://perinatology.com/Fetal%20Monitoring/Intrapartum%20Monitoring.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q10: The difference are that L1, as opposed to L2, does not take all the features into account, and that the L2 coefficients are smaller. It can be seen for example that for 'pathology', the features Width, Nzeros, Min and tendency are not used, and that for most of the features, L2 has smaller coefficients. We could expect it ahead because we know that L1 regression is doing feture selection, and that L2 shrinks the coefficients .**\n",
    "\n",
    "\n",
    "**Q11: Looking at the weights of the features, AC is most suggestive for normal fetuses and Variance is most suggestive for fetuses at risk. AC stands for accelerations- abrupt increase in FHR above baseline. According to the link above, absence of accelerations for more than 80 minutes correlates with increased neonatal morbidity. Therefore it is understood why the presence of accelerations indicates normality. \n",
    "Regarding Variance- According to the information in the link, persistently minimal or absent FHR variability appears to be the most significant intrapartum sign of fetal compromise. This also explains the importance of ASTV and ALTV (percentage of time with abnormal short/long term variability, respectively) in classification as 'pathology'. Physiologically it means that when there is a high HRV or a high percentage of time where the HRV is abnormal, the fetus is more likely to have a pathology.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a method that help us choose what we call *hyperparameters* of the model. This is also a method of regularization and it is called **validation**. There are several types of validation and here we will use *stratified K-fold cross validation*. The hyperparameters that we would like to choose are the norms that we want to train with and the regularization parameter. Again, we use stratification for the folds to prevent biased learning.\n",
    "\n",
    "Implement the function `cv_kfold` in `lin_classifier` module. We will use `X_train` as our training set that will be iteratively divided into $ K-1 $ training sets and one validation set. **Notice:** choose wisely where to apply `norm_standard` function to avoid information leakage in every iteration. In this function you should build a list of dictionaries called `validation_dict` where each element in the list contains a dictionary with 4 keys name: `C, penalty, mu and sigma`. For every pair of parameters (`C and penalty`) you will run $ K $ validations and `mu and sigma` will be calculated as the average loss and standard deviation over $ K $ folds respectively. Use the function `log_loss` from `sklearn.metrics` that was already imported in `lin_classifier`. One more thing, you will have to implement a simple modification to `pred_log` function using the `flag` argument. When this flag is set to `True`, the function should return the probabilities of the classes and not the classes themselves. This is the output that `log_loss` function expects to get.\n",
    "\n",
    "This function might take a while to perform depending on $ K $ and the number of regularization parameters you will choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([0.1, 1, 2, 5, 10, 20])  # make a list of up to 6 different values of regularization parameters and examine their effects\n",
    "K = 4  # choose a number of folds\n",
    "mode = 'MinMax'  # mode of nsd function\n",
    "val_dict = cv_kfold(X_train, y_train, C=C, penalty=['l1', 'l2'], K=K, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "for d in val_dict:\n",
    "    x = np.linspace(0, d['mu'] + 3 * d['sigma'], 1000)\n",
    "    plt.plot(x,stats.norm.pdf(x, d['mu'], d['sigma']), label=\"p = \" + d['penalty'] + \", C = \" + str(d['C'])) \n",
    "    plt.title('Gaussian distribution of the loss')\n",
    "    plt.xlabel('Average loss')\n",
    "    plt.ylabel('Probabilty density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now choose parameters according to the results and train you model with the **full training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10  # complete this part according to your best result\n",
    "penalty = 'l2'  # complete this part according to your best result\n",
    "logreg = LogisticRegression(solver='saga', multi_class='ovr', penalty=penalty, C=C, max_iter=10000)\n",
    "y_pred, w = pred_log(logreg, nsd(X_train, mode=mode), y_train, nsd(X_test, mode=mode))  # complete this function using nsd function\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "ax1 = plt.subplot(211)\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal','Suspect','Pathology'], yticklabels=['Normal','Suspect','Pathology'])\n",
    "ax1.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test, y_pred, average='macro'))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! So as you can see results did get better but not by that much but you got the feeling how to handle with data, what are the basics of learning and what are the real effects and applications of what you saw in the lectures. Now, one last thing: A possible reason for the poor improvements is that our data is probably not linearly separable and we used a linear classifier. There are two basic approaches for this kind of problem:\n",
    "The first one is to use non-linear classifier and the second one is to perform a transformation on our data so it will become linearly separable in another space. Here is an example of 2D data that can also visualize the problem and the second approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sthalles.github.io/assets/fisher-ld/feature_transformation.png\" width=600 align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the features were non-linearly transformed simply by squaring each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q12:** Look at the given figure above. Why was it reasonable to expect that squaring each feature would make the data linearly separable?\n",
    "\n",
    "**Q13:** Suggest another non-linear transformation that would make the data linearly separable so that the line that separates the two data types will be perpendicular to one of the new axes. Write the new two features (axes) **explicitly** as a function of $ (x_1,x_2) $. Use LaTex to write mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q12: The values in the graph have a relatively similar absolute value. Raising the square will cause the values to all have a positive sign and concentrate on a smaller range of values, while the red values will get pushed towards the 0 much more than the bigger values of the blue circle. The values will actually be seperated based on their distance from the origin, so that a linear line can be passed between them as shown in the second graph.** \n",
    "\n",
    "\n",
    "**Q13: We can convert the cartesian coordinatets into polar coordinations r and theta:\n",
    "\\begin{multline} r = \\sqrt{ {x_1^2} + {x_2^2} },\\space \\theta = tan(x_2/x_1) \\end{multline}\n",
    "So if we set the old x1 axis to be theta and old x2 axis to be r axis we will get 2 approximate lines, perpendicular to the r axis because the original dots had approximate same radius but different thets. We will than be able linearly separates the two data types with another line that is perpendicular to r axis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just to get the feeling of better results when we go non-linear, let's try the random forest classifier. All you have to do is just choose one of the modes of the `nsd` function and see if you got better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 92.49%\n",
      "F1 score is: 85.03%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "mode =  'MinMax'  # choose your method- We chose MinMax for comparison, though in this case standard gives better results\n",
    "clf = rfc(n_estimators=10)\n",
    "clf.fit(nsd(X_train, mode=mode), y_train)\n",
    "y_pred = clf.predict(nsd(X_test, mode=mode))\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal', 'Suspect', 'Pathology'],\n",
    "            yticklabels=['Normal', 'Suspect', 'Pathology'])\n",
    "ax.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test, y_pred, average='macro'))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
